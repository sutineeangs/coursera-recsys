{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quick-double",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quick-double",
    "outputId": "3e1f083c-a97a-40c9-8838-a73f6b45822b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies Imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk #for stemming process\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import json\n",
    "print('Dependencies Imported')\n",
    "# import test\n",
    "# print(test.test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chief-gathering",
   "metadata": {
    "id": "chief-gathering",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Coursera Course Recommendation System+WebApp (CountVectorization)\n",
    "# https://www.kaggle.com/code/sagarbapodara/coursera-course-recommendation-system-webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gLHJqhrJpyip",
   "metadata": {
    "id": "gLHJqhrJpyip",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "xrvDXmgxqEzu",
   "metadata": {
    "id": "xrvDXmgxqEzu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "_d8SexNAqug4",
   "metadata": {
    "id": "_d8SexNAqug4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords') #install stopwords\n",
    "#nltk.download('punkt')     #install \n",
    "#nltk.download()            #install\n",
    "#print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dh4u2vnp6Gt",
   "metadata": {
    "id": "1dh4u2vnp6Gt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def text_process(text):\n",
    "    # remove stop word\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = \" \".join([w for w in word_tokens if not w.lower() in stop_words])\n",
    "    # remove non alphabet \n",
    "    final = re.sub(r'[^a-zA-Z0-9,]', ' ', filtered_sentence)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "capable-affiliate",
   "metadata": {
    "id": "capable-affiliate",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getCourses():\n",
    "    # Read file\n",
    "#     data = pd.read_csv(\"Coursera.csv\")\n",
    "    data = pd.read_csv(\"../data/Coursera.csv\")\n",
    "    \n",
    "   \n",
    "    #column Course Name,Course Description,Skills\n",
    "    \n",
    "    # dup data\n",
    "    data['Course Name_2'] = data['Course Name']\n",
    "    data['Course Description_2'] = data['Course Description']\n",
    "    data['Skills_2'] = data['Skills']\n",
    "    #claen data \n",
    "    data['Course Name'] = data['Course Name'].apply(text_process)\n",
    "    data['Course Description'] = data['Course Description'].apply(text_process)\n",
    "    data['Skills'] = data['Skills'].apply(text_process)\n",
    "    ###\n",
    "\n",
    "    #data = data[['Course Name','Difficulty Level','Course Description','Skills']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "At9y5KGuok6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "At9y5KGuok6e",
    "outputId": "4e70d90f-49a9-4aaf-9724-6eb037a7d0ad",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df = getCourses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wGNEXW9O0y3g",
   "metadata": {
    "id": "wGNEXW9O0y3g",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3EEjq9wwoxEW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "3EEjq9wwoxEW",
    "outputId": "89006d3c-2556-4241-81ff-d899359b938e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #All Course Level\n",
    "# from wordcloud import WordCloud\n",
    "# from nltk.corpus import stopwords\n",
    "# #df_pos = df[df['Difficulty Level'] == 'Beginner']\n",
    "# pos_word_all = \" \".join(text for text in df['Course Description'])\n",
    "# reg = r\"[ก-๙a-zA-Z']+\"\n",
    "# fp = 'THSarabunNew.ttf'\n",
    "# stop_word = list(stopwords.words('english'))\n",
    "# wordcloud = WordCloud(stopwords=stop_word,\n",
    "#                       background_color = 'white',\n",
    "#                       max_words=150,\n",
    "#                       height = 2000,\n",
    "#                       width=4000,\n",
    "#                       font_path=fp,\n",
    "#                       regexp=reg).generate(pos_word_all)\n",
    "# plt.figure(figsize = (16,8))\n",
    "# plt.imshow(wordcloud)/\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2W4NO6M1vPT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "d2W4NO6M1vPT",
    "outputId": "abe1ab6b-31ad-456d-babc-be047d5cb042",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #Beginner Level\n",
    "# from wordcloud import WordCloud\n",
    "# from nltk.corpus import stopwords\n",
    "# df_pos = df[df['Difficulty Level'] == 'Beginner']\n",
    "# pos_word_all = \" \".join(text for text in df_pos['Course Description'])\n",
    "# reg = r\"[ก-๙a-zA-Z']+\"\n",
    "# fp = 'THSarabunNew.ttf'\n",
    "# stop_word = list(stopwords.words('english'))\n",
    "# wordcloud = WordCloud(stopwords=stop_word,\n",
    "#                       background_color = 'white',\n",
    "#                       max_words=150,\n",
    "#                       height = 2000,\n",
    "#                       width=4000,\n",
    "#                       font_path=fp,\n",
    "#                       regexp=reg).generate(pos_word_all)\n",
    "# plt.figure(figsize = (16,8))\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "EKl3LfCU1vCi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "EKl3LfCU1vCi",
    "outputId": "4f659840-00db-46cf-f696-415a94f0b1b6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Intermediate Level\n",
    "# #from wordcloud import WordCloud, STOPWORDS\n",
    "# from wordcloud import WordCloud\n",
    "# from nltk.corpus import stopwords\n",
    "# df_pos = df[df['Difficulty Level'] == 'Intermediate']\n",
    "# pos_word_all = \" \".join(text for text in df_pos['Course Description'])\n",
    "# reg = r\"[ก-๙a-zA-Z']+\"\n",
    "# fp = 'THSarabunNew.ttf'\n",
    "# stop_word = list(stopwords.words('english'))\n",
    "# wordcloud = WordCloud(stopwords=stop_word,\n",
    "#                       background_color = 'white',\n",
    "#                       max_words=150,\n",
    "#                       height = 2000,\n",
    "#                       width=4000,\n",
    "#                       font_path=fp,\n",
    "#                       regexp=reg).generate(pos_word_all)\n",
    "# plt.figure(figsize = (16,8))\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "jaoK8HeC2bYl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "jaoK8HeC2bYl",
    "outputId": "316a97b1-1052-484b-a4e9-98f2b9991e61",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #Advanced Level\n",
    "# from wordcloud import WordCloud\n",
    "# from nltk.corpus import stopwords\n",
    "# df_pos = df[df['Difficulty Level'] == 'Advanced']\n",
    "# pos_word_all = \" \".join(text for text in df_pos['Course Description'])\n",
    "# reg = r\"[ก-๙a-zA-Z']+\"\n",
    "# fp = 'THSarabunNew.ttf'\n",
    "# stop_word = list(stopwords.words('english'))\n",
    "# wordcloud = WordCloud(stopwords=stop_word,\n",
    "#                       background_color = 'white',\n",
    "#                       max_words=150,\n",
    "#                       height = 2000,\n",
    "#                       width=4000,\n",
    "#                       font_path=fp,\n",
    "#                       regexp=reg).generate(pos_word_all)\n",
    "# plt.figure(figsize = (16,8))\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "synthetic-modification",
   "metadata": {
    "id": "synthetic-modification",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getJsonData(df):\n",
    "    result = df.to_json(orient=\"records\")\n",
    "    parsed = json.loads(result)\n",
    "    return parsed\n",
    "#     return json.dumps(parsed, indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "central-exception",
   "metadata": {
    "id": "central-exception",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#defining the stemming function\n",
    "# def stem(text, ps):\n",
    "#     y=[]\n",
    "    \n",
    "#     # Stemming Process\n",
    "# #     ps = PorterStemmer()\n",
    "    \n",
    "#     for i in text.split():\n",
    "#         y.append(ps.stem(i))\n",
    "    \n",
    "#     return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "theoretical-diversity",
   "metadata": {
    "id": "theoretical-diversity",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Recommendation Function\n",
    "def recommend1(course):\n",
    "    course = text_process(course)\n",
    "    # Get data\n",
    "    courses = getCourses()\n",
    "    data = courses.copy()\n",
    "    \n",
    "    # Removing spaces between the words (Lambda funtions can be used as well)\n",
    "    data['course_name'] = data['Course Name'].copy()\n",
    "    data['Course Name'] = data['Course Name'].str.replace(' ',',')\n",
    "    data['Course Name'] = data['Course Name'].str.replace(',,',',')\n",
    "    data['Course Name'] = data['Course Name'].str.replace(':','')\n",
    "    \n",
    "    courses['course_name_index'] = data['Course Name'].str.replace(',',' ')\n",
    "    \n",
    "    data['Course Description'] = data['Course Description'].str.replace(' ',',')\n",
    "    data['Course Description'] = data['Course Description'].str.replace(',,',',')\n",
    "    data['Course Description'] = data['Course Description'].str.replace('_','')\n",
    "    data['Course Description'] = data['Course Description'].str.replace(':','')\n",
    "    data['Course Description'] = data['Course Description'].str.replace('(','')\n",
    "    data['Course Description'] = data['Course Description'].str.replace(')','')\n",
    "\n",
    "    #removing paranthesis from skills columns \n",
    "    data['Skills'] = data['Skills'].str.replace('(','')\n",
    "    data['Skills'] = data['Skills'].str.replace(')','')\n",
    "    \n",
    "\n",
    "    \n",
    "    data['tags'] = data['Course Name'] + data['Difficulty Level'] + data['Course Description'] + data['Skills']\n",
    "    \n",
    "    new_df = data[['Course Name','tags']]\n",
    "    new_df['tags'] = data['tags'].str.replace(',',' ')\n",
    "    new_df['Course Name'] = data['Course Name'].str.replace(',',' ')\n",
    "    new_df.rename(columns = {'Course Name':'course_name'}, inplace = True)\n",
    "    new_df['tags'] = new_df['tags'].apply(lambda x:x.lower()) #lower casing the tags column\n",
    "    \n",
    "    # Text Vectorization\n",
    "    cv = CountVectorizer(max_features=5000,stop_words='english')\n",
    "    vectors = cv.fit_transform(new_df['tags']).toarray()\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    #defining the stemming function\n",
    "    def stem(text):\n",
    "        y=[]\n",
    "    \n",
    "        for i in text.split():\n",
    "            y.append(ps.stem(i))\n",
    "    \n",
    "        return \" \".join(y)\n",
    "    new_df['tags'] = new_df['tags'].apply(stem) #applying stemming on the tags column\n",
    "    similarity = cosine_similarity(vectors)\n",
    "\n",
    "    course_index = new_df[new_df['course_name'] == course].index[0]\n",
    "\n",
    "    distances = similarity[course_index]\n",
    "    course_list = sorted(list(enumerate(distances)),reverse=True, key=lambda x:x[1])[1:11]\n",
    "    \n",
    "    course_names = []\n",
    "    for i in course_list:\n",
    "        course_names.append(new_df.iloc[i[0]].course_name)\n",
    "        #print(new_df.iloc[i[0]].course_name)\n",
    "    #print(course_names)\n",
    "\n",
    "    #return raw data\n",
    "    data_return = courses.set_index('course_name_index').loc[course_names].reset_index().drop_duplicates(subset=['course_name_index'])\n",
    "    data_return['Course Name'] = data_return ['Course Name_2']\n",
    "    data_return['Course Description'] = data_return['Course Description_2'] \n",
    "    data_return['Skills'] = data_return['Skills_2']\n",
    "    data_return.drop(['Course Name_2','Course Description_2','Skills_2'], axis = 1,inplace = True) \n",
    "    return data_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "answering-olympus",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "answering-olympus",
    "outputId": "a58fa4db-4ffc-489f-dad2-ef2a28fad7a5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# recommend1('Write A Feature Length Screenplay For Film Or Television')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "saving-constitutional",
   "metadata": {
    "id": "saving-constitutional",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Coursera Course Recommendation Engine (TfidfVectorizer)\n",
    "# https://www.kaggle.com/code/brijlaldhankour/coursera-course-recommendation-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "thirty-reply",
   "metadata": {
    "id": "thirty-reply",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating autocpt arguments \n",
    "def func(pct, allvalues): \n",
    "    absolute = int(pct / 100.*np.sum(allvalues)) \n",
    "    return \"{:.1f}%\\n({:d} g)\".format(pct, absolute) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "noted-charles",
   "metadata": {
    "id": "noted-charles",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plotGraph1():\n",
    "    # Get data\n",
    "    courses = getCourses()\n",
    "    df = courses.copy()\n",
    "    \n",
    "    # This dataframe contains some useless columns which must be dropped for a better analytics result\n",
    "    uc = ['Course URL']  # uc means useless columns\n",
    "    df = df.drop(columns=uc)\n",
    "    \n",
    "    # Data Inferencing\n",
    "    r,c = df.shape\n",
    "    #======================== Plot Graph =====================================#\n",
    "    # Data Visualization\n",
    "    datavis = df['Difficulty Level'].value_counts()\n",
    "    datavis\n",
    "    mag = datavis.index\n",
    "    data = datavis.values\n",
    "    explode = (0.1,0.0,0.0,0.0,0.0) \n",
    "\n",
    "    # Creating color parameters \n",
    "    colors = (\"lightblue\",\"crimson\",\"yellow\",\"green\",\"violet\") \n",
    "\n",
    "    # Wedge properties \n",
    "    wp = { 'linewidth' : 1, 'edgecolor' : \"white\" } \n",
    "\n",
    "    # Creating plot \n",
    "    fig, ax = plt.subplots(figsize =(15, 10)) \n",
    "    wedges, texts, autotexts = ax.pie(data,  \n",
    "                                      autopct = lambda pct: func(pct, data), \n",
    "                                      explode = explode,  \n",
    "                                      labels = mag, \n",
    "                                      shadow = True, \n",
    "                                      colors = colors, \n",
    "                                      startangle = 90, \n",
    "                                      wedgeprops = wp, \n",
    "                                      textprops = dict(color =\"black\")) \n",
    "\n",
    "    # Adding legend \n",
    "    ax.legend(wedges, mag, \n",
    "              title =\"Values\", \n",
    "              loc =\"center left\", \n",
    "              bbox_to_anchor =(1, 0, 0.5, 1)) \n",
    "\n",
    "    plt.setp(autotexts, size = 10, weight =\"bold\") \n",
    "    ax.set_title(\"Payment type of course\\n\",size=19) \n",
    "\n",
    "    # show plot \n",
    "    plt.show()\n",
    "    #=========================================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "toxic-federal",
   "metadata": {
    "id": "toxic-federal",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plotGraph2():\n",
    "    # Get data\n",
    "    courses = getCourses()\n",
    "    df = courses.copy()\n",
    "    \n",
    "    # This dataframe contains some useless columns which must be dropped for a better analytics result\n",
    "    uc = ['Course URL']  # uc means useless columns\n",
    "    df = df.drop(columns=uc)\n",
    "    \n",
    "    # Data Inferencing\n",
    "    r,c = df.shape\n",
    "    #======================== Plot Graph =====================================#\n",
    "    df['Course Rating'].value_counts()\n",
    "    df = df[df['Course Rating'] != 'Not Calibrated']\n",
    "    df['Course Rating'] = df['Course Rating'].astype(float)\n",
    "    plt.figure(figsize=(18,7))\n",
    "    sns.countplot(data=df,x='Course Rating',palette='plasma')\n",
    "    plt.xlabel('Course Ratings',fontsize='16',color='blue')\n",
    "    plt.ylabel('Number of courses',fontsize='16',color='blue')\n",
    "    plt.xticks(fontsize='14',color='green')\n",
    "    plt.yticks(fontsize='14',color='red')\n",
    "    plt.title(\"Count of course types\\n\",fontsize=24,fontweight='bold',color='indigo')\n",
    "    #=========================================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "comprehensive-astronomy",
   "metadata": {
    "id": "comprehensive-astronomy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Recommendation Function\n",
    "def recommend2(course_title):\n",
    "    \n",
    "    course_title = text_process(course_title)\n",
    "\n",
    "    # Get data\n",
    "    courses = getCourses()\n",
    "    df = courses.copy()\n",
    "    \n",
    "    # This dataframe contains some useless columns which must be dropped for a better analytics result\n",
    "    uc = ['Course URL']  # uc means useless columns\n",
    "    df = df.drop(columns=uc)\n",
    "    \n",
    "    # Data Inferencing\n",
    "    r,c = df.shape\n",
    "    #print(\"Number of customers  = \",r)\n",
    "    #print(\"Number of parameters = \",c)\n",
    "    #print(df.info())\n",
    "    #print(\"Are there any missing values in the dataset ?\",df.isna().values.any())\n",
    "\n",
    "    # complete summary of dataset\n",
    "    #df.describe().T\n",
    "    \n",
    "    df = df[df['Course Rating'] != 'Not Calibrated']\n",
    "    df['Course Rating'] = df['Course Rating'].astype(float)\n",
    "    #=========================================================================#\n",
    "    # Filtering required data\n",
    "    df2 = df[df['Course Rating'] > 4.0]\n",
    "    #=========================================================================#\n",
    "    # Making NLP Model for Recommendation Engine\n",
    "    cv=TfidfVectorizer()\n",
    "    tfidf_matrix=cv.fit_transform(df['Course Name'])\n",
    "    course_user = df.pivot_table(columns='Course Name',values='Course Rating')\n",
    "    course_user.head()\n",
    "    df = df.rename(columns={'Course Name':'course_title'})\n",
    "    \n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    indices=pd.Series(df.index,index=df['course_title'])\n",
    "    titles=df['course_title']\n",
    "\n",
    "    # Running Recommendation Engine on variety of course genres\n",
    "    idx = indices[course_title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    course_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    #return raw data\n",
    "    data_return = courses.loc[titles.iloc[course_indices].index].drop_duplicates(subset=['Course Name'])\n",
    "    data_return['Course Name'] = data_return ['Course Name_2']\n",
    "    data_return['Course Description'] = data_return['Course Description_2'] \n",
    "    data_return['Skills'] = data_return['Skills_2']\n",
    "    data_return.drop(['Course Name_2','Course Description_2','Skills_2'], axis = 1,inplace = True) \n",
    "    return data_return\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fleet-island",
   "metadata": {
    "id": "fleet-island",
    "outputId": "250885d9-d82b-4492-c29a-cc133bce1e1b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test functions\n",
    "#recommend1('Business Strategy Business Model Canvas Analysis with Miro') \n",
    "\n",
    "# plotGraph1()\n",
    "# plotGraph2()\n",
    "\n",
    "# TOPIC : 1 TV shows and telecast\n",
    "# print(\"--------------- Similar courses to your search --------------:\\n\")\n",
    "# recommend2('Write A Feature Length Screenplay For Film Or Television')\n",
    "\n",
    "# TOPIC : 2 Database and related courses\n",
    "# print(\"--------------- Similar courses to your search --------------:\\n\")\n",
    "# recommend2('Retrieve Data using Single-Table SQL Queries')\n",
    "\n",
    "# TOPIC : 3 Finance related\n",
    "# print(\"--------------- Similar courses to your search --------------:\\n\")\n",
    "# recommend2('Finance for Managers')\n",
    "# getJsonData(recommend2('Finance for Managers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "uniform-strengthening",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uniform-strengthening",
    "outputId": "966b9ab6-5c94-4892-cbc3-2946f875a7a8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# recommend1('Finance for Managers') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cxjLwEl8wbPO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "id": "cxjLwEl8wbPO",
    "outputId": "fd78d643-53d1-4818-9ee6-e550dad54dd2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# recommend2('Finance for Managers') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GUKlsaR3yiay",
   "metadata": {
    "id": "GUKlsaR3yiay",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-fraction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "recsys_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
